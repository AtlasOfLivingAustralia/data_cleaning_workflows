---
title: "Download ALA by taxon"
author: "Payal Bal, Fonti Kar"
date: "2022-08-30"
output: html_document
editor_options: 
  chunk_output_type: console
---


Below I have split up the `download_ALA_bytaxon` workflow into chunks. I provided a streamlined / `galah`-friendly version directly below the original NESP code.I've also included some notes under each heading and as # comments throughout.

I think the major difference between the NESP bugs and current galah workflow is that NESP bugs supplies a species likes from AFD `afd_taxonomy$PHYLUM` to query ALA, wheras galah just filters by ALL INVERTEBRATES and then downloads the occurences. I can certainly query ALA via galah with the same species lis and see how close the output is and where the discrepancies are with NESP bugs workflow

Some of the latter parts of this workflow involves collapsing down the lists of download (organised by phylum).  I assumed this was a product for parallel processing using `future` so I didn't write streamlined parts for these sections as output depends on user and `galah` provides one long tibble for all phylum. 

**Cautionary note: It is not advised to parallelise downloads from `galah`** as this has a very high chance of stalling BioCache API and the user will get kicked out

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
```

### Set working environment 

```{r}
rm(list = ls())

# install.packages("pacmac")
pacman::p_load(data.table, galah, sp, raster, future, future.apply)
```

### Setting up server paths

An .Rproj can alleviate some of this path-setting steps but everyone's computer is different. I like the downloads folder code

```{r}
## Server paths
source(file.path(getwd(),"nesp_bugs", "scripts/get_ala_taxondata.R"))
bugs_dir = "/tempdata/research-cifs/uom_data/nesp_bugs_data"
output_dir = file.path(bugs_dir, "outputs")

# ## Local paths
# source(file.path(getwd(), "scripts/get_ala_taxondata.R"))
# output_dir = "/Volumes/uom_data/nesp_bugs_data/outputs"

ala_dir <- file.path(bugs_dir, "ALA", "download_bytaxon")
if(!dir.exists(ala_dir)) dir.create(ala_dir)
```

### Configuring ALA account
```{r}
galah_config(email = "fonti.kar@gmail.com", 
             atlas = "Australia",
             download_reason_id = 10 # testing reason
             )
```

### Reading in a cleaned AFD species list

```{r}
## Load AFD taxonomic checklist ####
afd_taxonomy <- fread("output/afd_splist_full.csv")
afd_taxon <- unique(afd_taxonomy$PHYLUM) 
length(afd_taxon)
```

### Count records and write as .csv 

`taxon.counts()` calls for `get_ala_taxondata.R` to get a tally of each taxon, but `get_ala_taxondata.R` actually has download occurrence records first, cleans it up a bit and then tallies. More notes below:

`get_ala_taxondata.R` downloads occurrence records depending on: 

1. basis of record if `specimens_only = TRUE`
    e.g. 
    "PreservedSpecimen", "LivingSpecimen", 
    "MachineObservation", "EnvironmentalDNA","GenomicDNA"
2. [fields](https://biocache.ala.org.au/ws/index/fields), this version is better for the eyes: https://biocache.ala.org.au/fields
3. [Darwin Core fields](https://biocache.ala.org.au/ws/index/fields) via ALA4R::occurrences::`qa` argument. 
   `qa` was preferred over `extra = "assertions"` because `extra = "assertions"` only lists one assertion, even if the record has multiple.
    e.g.
    "el1055", "el1056", #endemism 
    "el682", #migratory
    "disposition", 
    "assertions","assertions_unchecked", 
    "raw_data_generalizations",
    "duplicate_record", "duplicate_status", 
    "sensitive","taxonomic_issue")

Some data cleaning steps in the `get_ala_taxondata.R`: 

1. Removes rows where `latitude` and `latitude` are NA
```{r}
taxon.counts <- lapply(afd_taxon,
                       function(x){
                         tmp <- tryCatch(expr = get_ala_taxondata(x,
                                                                  get_counts_only = TRUE,
                                                                  specimens_only = TRUE,
                                                                  dst = ala_dir),
                                         error = function(e){ 
                                           print(paste("\nNot run: no records for", x))
                                           
                                           cat(paste(x, "\n"),
                                               file = nodatalog, 
                                               append = T)
                                         })
                       })

taxon.counts <- as.data.frame(taxon.counts)
colnames(taxon.counts) <- afd_taxon
write.csv(taxon.counts, file.path(output_dir, "taxon_counts.csv"))
```

### {galah} alternative for getting counts for each Invertebrate Phylum

Fields for [`basisOfRecord`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/basisOfRecord)

**Note:** ALA has collapsed `EnvironmentalDNA`, `GenomicDNA`" into `Material_Sample` 
ALA has added [`contentTypes`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/contentTypes) to further delineate these categories

```{r}
# Equvialent to n.all 
# Obtain counts for each phylum
galah_call() |>
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier # Filters NOT Chordata
  ) |> 
  galah_group_by(phylum) |>
  atlas_counts() -> n.all 

# To view more rows
n.all |> print(n = 50) # Change n for view more

# This is equivalent to n.obs
galah_call() |>
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier,
    basisOfRecord == "HUMAN_OBSERVATION" # Filters to human observed
  ) |> 
  galah_group_by(phylum) |> 
  atlas_counts() -> n.obs

n.obs |> print(n = 50)

# This is equivalent to n.spec
# If need to filter basis of record, equivalent to specimen_only = TRUE in `get_ala_taxondata`
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")
galah_call() |>
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only # Filters to specimen only
  ) |> 
  galah_group_by(phylum) |> 
  atlas_counts() -> n.spec

n.spec |> print(n =)
```

### {galah} alternative for filtering records by assertions

Assertions are used to filter down records ensure the data is fit for use. 
The `{galah}` code below is a **temporary** solution for the current version of the R package.
I believe there will be a formalised solution in the next release. Watch this space.


Currently, users can filter records using `galah_filter()` as below.  
Replacing `UNKNOWN_KINGDOM` with whatever assertion name youâ€™d like to use. 
Alternatively, to exclude by multiple assertions, use `c()` 

This method of filtering can also applied to downloading records as well as obtaining counts.

```{r}
galah_call() |>
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    assertions != UNKNOWN_KINGDOM
  ) |> 
  galah_group_by(phylum) |> 
  atlas_counts() 


IA_assertions <- c("UNKNOWN_KINGDOM", "TAXON_MATCH_NONE",
                   "COORDINATE_PRECISION_MISMATCH", "MISSING_GEODETICDATUM")

galah_call() |> 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    assertions != IA_assertions
  ) |> 
  galah_group_by(phylum) |> 
  atlas_counts()
```

### `{galah}` alternative download occurence records

Just searching each field from the `fields` vector above here https://biocache.ala.org.au/fields to make a new vector that works with the current API system. **Note: to avoid parallel process this step to avoid crashes to API system**

Unable to find equivalent fields for: 
- `taxonomic_kosher`
- `catalogue_number`

Some fields did not have an exact match but I took my best guess to find a match i.e. original ~ best guess

-raw_taxon_name ~ raw_scientificName
-raw_datum ~ raw_geodeticDatum
-taxon_name ~ scientificName
-common_name ~ vernacularName
-rank ~ taxonRank
-state ~ locality
-min_elevation_d not sure what _d represents ~ minimumElevationInMeters
-max_elevation_d not sure what _d represents ~ maximumElevationInMeters
-min_depth_d not sure what _d represents  ~  minimumDepthInMeters
-max_depth_d not sure what _d represents ~ maximumDepthInMeters
-collector ~ recordedBy
-occurrence_date ~ eventDate
-geospatial_kosher ~ spatiallyValid

```{r}
# Limiting my download to a specific year for now
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")

# Fields that we want
fields <-  c("id","dataResourceUid","dataResourceName",
             "institutionID","institutionUid", "institutionName", # Not sure what is the difference between ID and Uid
             "collectionID", "collectionUid","collectionName",
             "contentTypes", 
             "license", 
             "taxonConceptID",
             "raw_scientificName" ,"raw_vernacularName", 
             "scientificName", 
             "vernacularName", 
             "taxonRank", 
             "kingdom","phylum","class","order", 
             "family","genus","species","subspecies",
             "institutionCode","collectionCode",
             "locality", 
             "raw_geodeticDatum", 
             "raw_decimalLatitude","raw_decimalLongitude",
             "decimalLatitude","decimalLongitude",
             "coordinatePrecision","coordinateUncertaintyInMeters",
             "country","stateProvince", 
             "cl959","cl21","cl1048",
             "minimumElevationInMeters", "maximumElevationInMeters", 
             "minimumDepthInMeters", "maximumDepthInMeters",
             "individualCount",
             "recordedBy", 
             "eventDate", 
             "year","month",
             "verbatimEventDate",
             "basisOfRecord","raw_basisOfRecord",
             "occurrenceStatus",
             "raw_sex", "sex",
             "preparations",
             "outlierLayer",
             "spatiallyValid"
             )

galah_call() |> 
  galah_filter(
    year == 2022,
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only # Filters to specimen only
  ) |> 
  galah_select(fields) |> 
  atlas_occurrences() -> invert_2022

# saveRDS(invert_2022, "output/invert_2022.rds")
invert_2022 <- readRDS("output/invert_2022.rds")

## Doing the mass download

galah_call() |> 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, # Filters NOT Chordata
    basisOfRecord == specimen_only # Filters to specimen only
  ) |> 
  galah_select(fields, assertions) |>  
  atlas_occurrences() -> invert_all

# saveRDS(invert_all, "output/invert_all.rds")
invert_all <- readRDS("output/invert_all.rds")
```

Benchmarking the download time to compare with ALA4R parallel method

```{r}
# devtools::install_github("jabiru/tictoc")
library(tictoc)

tic("galah download")
galah_call() |> 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, # Filters NOT Chordata
    basisOfRecord == specimen_only # Filters to specimen only
  ) |> 
  galah_select(fields) |>  
  atlas_occurrences() -> invert_all
toc()

# galah download: 1949.915 sec elapsed
1949.915/60 # 32 mins
```

### Checks of downloads 

No changes made here as a lot of the code below here applies to a list

```{r}

## Check field names across all downnloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
dat_cols <- names(readRDS(ala[1])$data)
for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field names as per specified list = "),
          all(dat_cols==names(f)))
}

all(fields %in% names(f$data)) ## because names are different even if fields correspond
fields[which(fields %!in% names(f$data))]


## Check field classes across all downloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
ala <- names(sort(sapply(ala, file.size)))
f <- readRDS(ala[1])$data
dat_cols <- names(f)
coltypes <- sapply(f[,..dat_cols], class)

for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  f_coltypes <- sapply(f[,..dat_cols], class)
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field classes as per specified list = "),
          all(coltypes==f_coltypes))
}
```

### Some data cleaning steps that are in get_ala_taxondata.R

```{r}
invert_2022_test <- invert_2022
invert_2022_test$decimalLatitude[sample(nrow(invert_2022_test),5)] <-NA # replace 5 obs with NA for a test

# Excluding rows with no coordinate data (test)
invert_2022_test |> filter(is.na(decimalLatitude) ) |> nrow() # 5 rows are detected gets excluded

invert_2022_test |> filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) #nrow = 128

# Excluding rows with no coordinate data
invert_2022 |> filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) -> invert_2022


# Treating date as date and coordinates as numeric
invert_2022 |> mutate(verbatimEventDate = lubridate::dmy(verbatimEventDate),
                      decimalLatitude = as.numeric(decimalLatitude),
                      decimalLongitude = as.numeric(decimalLongitude),
                      verbatimLatitude = as.numeric(verbatimLatitude),
                      verbatimLongitude = as.numeric(verbatimLongitude)
                      )
```


### Merge downloaded data

Arthropods is a big data set so its being treated as a `data.table` obj
Looks like counts are reformatted from list to a data frame and the class of each variable is assigned to coltypes

```{r}
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)

## Sort files by size
ala <- names(sort(sapply(ala, file.size), decreasing = TRUE))

## Create data table with Arthropoda dataset (biggest dataset)
f <- readRDS(ala[1])
dat_cols <- names(f$data)
dat_counts <- t(as.data.frame(f$counts))
dat_counts
colnames(dat_counts) <- names(f$counts)
ala_merged <- as.data.table(f$data)
dim(ala_merged)
coltypes <- sapply(ala_merged[,..dat_cols], class)
rm(f)
```

### Merge in remaining datasets

Here the code deals with everything except Arthopods.
By merging I think its just collapsing down the lists of each phylum
Some code to deal with mismatched column classes from different phylum.
I suspect this might not be an issue as `{galah}` downloads in one giant tibble, but requires testing and splitting phyla into lists

```{r}
for (i in 2:length(ala)) {
  f <- ala[i]
  f <- readRDS(f)
  c <- t(as.data.frame(f$counts))
  f <- as.data.table(f$data)
  
  message(cat("Processing dataset ", i, " :", ala[i], " ..."))
  
  dat_counts <- rbind(dat_counts, c)      
  message(cat("Total number of clean records: "),
          sum(dat_counts[,4]))
  
  message(cat("Matching column classes..."))
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  f_mismatch <- which(!(coltypes == f_coltypes))
  for (k in f_mismatch) set(f, j = k, value = eval(parse(text=paste0("as.", coltypes[k], "(f[[k]])"))))
  
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  message(cat("Checking columns classes are same... "),
          all(f_coltypes==coltypes))
  
  message(cat("Merging dataset ..."))
  ala_merged <- rbind(ala_merged, f, use.names = TRUE, fill=TRUE)
  message(cat("Dimensions of merged data: "),
          dim(ala_merged)[1])
  message("\n")
  rm(c,f) 
}

```

### Post merge checks

No changes made here

```{r}
## Check
message(cat("#rows in merged data = sum of cleaned records : "),
        nrow(ala_merged) == sum(dat_counts[,4]))
```

### Save counts and occurences outputs as seperate .rds

```{r}
rownames(dat_counts) <- gsub(".rds", "", basename(ala))
saveRDS(dat_counts, file = file.path(output_dir, "ala_counts.rds"))

saveRDS(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".rds")))
write.csv(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".csv")), row.names = FALSE)

```

#### EXTRA

In next version of `{galah}`, users will be able to obtain a list of assertions. Currently you can view the assertions in messy form [here](https://github.com/AtlasOfLivingAustralia/biocache-store/blob/5c55b021d283026868924f4c3c88a3061ec06956/src/main/scala/au/org/ala/biocache/vocab/AssertionCodes.scala)

```{r}
# ## Explore ALA fields
# ala_fields("occurrence", as_is=TRUE)
# names(ala_fields("occurrence"))
# ala_fields("occurrence")$name
# "assertions" %in% ala_fields("occurrence")$name
# ala_fields()$name[grep("assertions", ala_fields("occurrence")$name)] 
# ala_fields()$description[grep("assertions", ala_fields("occurrence")$name)]

## ALA assertions
ala_fields("assertions",as_is=TRUE)
write.csv(ala_fields("assertions",as_is=TRUE), file = "./output/assertions.csv")
```

