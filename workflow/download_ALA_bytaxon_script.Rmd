---
title: "Download ALA by taxon"
author: "Payal Bal, Fonti Kar"
date: "2022-08-30"
output: html_document
editor_options: 
  chunk_output_type: console
---

Below I have split up the `download_ALA_bytaxon` workflow into chunks. I provided a streamlined / `galah`-friendly version directly below the original NESP code.I've also included some notes under each heading and as # comments throughout.

I think the major difference between the NESP bugs and current galah workflow is that NESP bugs workflow supplies a species list from AFD `afd_taxonomy$PHYLUM` to query ALA, whereas `galah` filters by ALL INVERTEBRATES first and then downloads the occurrences. We recommend processing the download and cross match with AFD after the download.

Some of the latter parts of NESP bugs workflow involves collapsing down the lists of download (organised by phylum).  I assumed this was a product for parallel processing using `future` so I didn't write streamlined these sections.  

**Cautionary note: It is advised to NOT to parallelise downloads from `galah`** as this has a very high chance of stalling BioCache API and the user will get kicked out. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
```

### Set working environment 

```{r}
rm(list = ls())

# install.packages("pacmac")
pacman::p_load(data.table, galah, dplyr, CoordinateCleaner, sf, ozmaps, job, arrow, purrr)
```

### Setting up server paths

An .Rproj can alleviate some of this path-setting steps but everyone's computer is different. 

```{r}
## Server paths
source(file.path(getwd(),"nesp_bugs", "scripts/get_ala_taxondata.R"))
bugs_dir = "/tempdata/research-cifs/uom_data/nesp_bugs_data"
output_dir = file.path(bugs_dir, "outputs")

# ## Local paths
# source(file.path(getwd(), "scripts/get_ala_taxondata.R"))
# output_dir = "/Volumes/uom_data/nesp_bugs_data/outputs"

ala_dir <- file.path(bugs_dir, "ALA", "download_bytaxon")
if(!dir.exists(ala_dir)) dir.create(ala_dir)
```

### Configuring ALA account
```{r}
galah_config(email = "fonti.kar@gmail.com", 
             atlas = "Australia",
             download_reason_id = 10 # testing reason
             )
```

### Reading in a cleaned AFD species list

```{r}
## Load AFD taxonomic checklist ####
afd_taxonomy <- fread("output/afd_splist_full.csv")
afd_taxon <- unique(afd_taxonomy$PHYLUM) 
length(afd_taxon)
```

### Count records and write as .csv 

`taxon.counts()` calls for `get_ala_taxondata.R` to get a tally of each taxon, but `get_ala_taxondata.R` actually has download occurrence records first, cleans it up a bit and then tallies. More notes below:

`get_ala_taxondata.R` downloads occurrence records depending on: 

1. basis of record if `specimens_only = TRUE`
    e.g. 
    "PreservedSpecimen", "LivingSpecimen", 
    "MachineObservation", "EnvironmentalDNA","GenomicDNA"
2. [fields](https://biocache.ala.org.au/ws/index/fields), this version is better for the eyes: https://biocache.ala.org.au/fields
3. [Darwin Core fields](https://biocache.ala.org.au/ws/index/fields) via ALA4R::occurrences::`qa` argument. 
   `qa` was preferred over `extra = "assertions"` because `extra = "assertions"` only lists one assertion, even if the record has multiple.
    e.g.
    "el1055", "el1056", #endemism 
    "el682", #migratory
    "disposition", 
    "assertions","assertions_unchecked", 
    "raw_data_generalizations",
    "duplicate_record", "duplicate_status", 
    "sensitive","taxonomic_issue")

Some data cleaning steps in the `get_ala_taxondata.R`: 
1. Removes rows where `latitude` and `latitude` are NA
```{r}
taxon.counts <- lapply(afd_taxon,
                       function(x){
                         tmp <- tryCatch(expr = get_ala_taxondata(x,
                                                                  get_counts_only = TRUE,
                                                                  specimens_only = TRUE,
                                                                  dst = ala_dir),
                                         error = function(e){ 
                                           print(paste("\nNot run: no records for", x))
                                           
                                           cat(paste(x, "\n"),
                                               file = nodatalog, 
                                               append = T)
                                         })
                       })

taxon.counts <- as.data.frame(taxon.counts)
colnames(taxon.counts) <- afd_taxon
write.csv(taxon.counts, file.path(output_dir, "taxon_counts.csv"))
```

### {galah} for getting counts for each Invertebrate Phylum

Fields for [`basisOfRecord`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/basisOfRecord)

**Note:** ALA has collapsed `EnvironmentalDNA`, `GenomicDNA`" into `Material_Sample` 
ALA has added [`contentTypes`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/contentTypes) to further delineate these categories

```{r}
# Equvialent to n.all 
# Obtain counts for each phylum
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier # Filters NOT Chordata
  ) %>% 
  galah_group_by(phylum) %>%
  atlas_counts() -> n.all 

# To view more rows
n.all %>% print(n = 50) # Change n for view more

# This is equivalent to n.obs
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier,
    basisOfRecord == "HUMAN_OBSERVATION" # Filters to human observed
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() -> n.obs

n.obs %>% print(n = 50)

# This is equivalent to n.spec
# If need to filter basis of record, equivalent to specimen_only = TRUE in `get_ala_taxondata`
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only # Filters to specimen only
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() -> n.spec

# Format data to save as a .csv
# Rename
n.all %>% rename(n.all = count) -> n.all
n.obs %>% rename(n.obs = count) -> n.obs
n.spec %>% rename(n.spec = count) -> n.spec

# Join all counts
left_join(n.all, n.obs) %>% 
  left_join(., n.spec) -> taxon_counts

# Save as csv
write.csv(taxon.counts, "output/taxon_counts.csv")
```

### {galah} for filtering records by assertions

Assertions are used to filter down records ensure the data is fit for use. 
There was a need to export this, PB sent us a spreadsheet called `qa_assertions.xlsx`, output saved from the ALA4R

The only assertion used to exclude data was `identificationIncorrect` 

To display all assertion fields use:

```{r}
assertions <- show_all("assertions") %>% print(n = 250)

# Save assertion data as .csv
write_csv(assertions, "output/assertions.csv")
```

Users can filter records using `galah_filter()` as below.  
Replacing `identificationIncorrect` with whatever assertion name youâ€™d like to use. 
Alternatively, to exclude by multiple assertions, use `c()` 

This method of filtering can also applied to downloading records as well as obtaining counts.

```{r}
# For single assertion exclusions
galah_call()  %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    identificationIncorrect == FALSE
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() 

# For exclusions using multiple assertions
IA_assertions <- c("UNKNOWN_KINGDOM", "identificationIncorrect",
                   "COORDINATE_PRECISION_MISMATCH", "MISSING_GEODETICDATUM")

galah_call() %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    assertions != IA_assertions
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts()
```
#### Download with assertion fields

NOTE THIS IS CURRENTLY NOT WORKING 
```{r}
galah_call() %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    identificationIncorrect == FALSE,
    year == 2022)  %>% # Limiting to 2022 for now
  galah_select(group = "assertions") %>%
  atlas_occurrences() -> inverts_assertions
```

### {galah} download occurrence records

Just searching each field from the `fields` vector above here https://biocache.ala.org.au/fields to make a new vector that works with the current API system. **Note: to avoid parallel process this step to avoid crashes to API system**

Unable to find equivalent fields for: 

- `taxonomic_kosher`

Some fields did not have an exact match but I took my best guess to find a match i.e. original ~ best guess

-raw_taxon_name ~ raw_scientificName
-raw_datum ~ raw_geodeticDatum
-taxon_name ~ scientificName
-common_name ~ vernacularName
-rank ~ taxonRank
-state ~ locality
-min_elevation_d not sure what _d represents ~ minimumElevationInMeters
-max_elevation_d not sure what _d represents ~ maximumElevationInMeters
-min_depth_d not sure what _d represents  ~  minimumDepthInMeters
-max_depth_d not sure what _d represents ~ maximumDepthInMeters
-collector ~ recordedBy
-occurrence_date ~ eventDate
-geospatial_kosher ~ spatiallyValid

#### Investigate fields are there

I think safer to split into 3, 2 parts is hit and miss

```{r}
ALA_fields <- show_all(fields)

ALA_fields %>% print(n = 100)

# Search the fields
search_all(fields, "coordinate") 

# Fields that IA want
IA_fields<- c("recordID","dataResourceUid","dataResourceName",
              "institutionID","institutionName",
              "collectionID", "collectionUid","collectionName",
              "contentTypes", 
              "license", 
              "taxonConceptID",
              "raw_scientificName" ,"raw_vernacularName", 
              "scientificName", 
              "vernacularName", 
              "taxonRank", 
              "kingdom","phylum","class","order", 
              "family","genus","species","subspecies",
              "institutionCode","collectionCode",
              "locality", 
              "raw_geodeticDatum", 
              "raw_decimalLatitude","raw_decimalLongitude", 
              "decimalLatitude","decimalLongitude",
              "coordinatePrecision","coordinateUncertaintyInMeters",
              "country","stateProvince", 
              "cl959","cl21","cl1048",
              "minimumElevationInMeters", "maximumElevationInMeters", 
              "minimumDepthInMeters", "maximumDepthInMeters",
              "individualCount",
              "recordedBy",
              "eventDate",
              "year","month",
              "verbatimEventDate",
              "basisOfRecord","raw_basisOfRecord",
              "occurrenceStatus",
              "raw_sex", "sex",
              "preparations",
              "outlierLayer",
              "spatiallyValid", "catalogNumber") 

length(IA_fields)

# Split into 2 parts
IA_fields_split <- split(IA_fields, ceiling(seq_along(IA_fields)/20))

# We want to eventually join by recordID so need to add this to the 2-4 parts
IA_fields_split$`2` <- c("id", IA_fields_split$`2`)
IA_fields_split$`3` <- c("id", IA_fields_split$`3`)
```


### {galah} download: user supplied list of taxa

I realized there are some inverts that are in Chordata, so we definitely don't want to exclude all Chordates. This method would also work with taxon counts above

```{r}
search_taxa(afd_taxon) # Issues with ACANTHOCEPHALA its technically a Phylum but recorded as a class in here https://bie.ala.org.au/species/NZOR-6-53977

# Currently working with identifier for ACANTHOCEPHALA
galah_call() %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  atlas_counts()

# Current workaround to get taxa in AFD list
afd_taxon_phylums <- afd_taxon[-which(afd_taxon == "ACANTHOCEPHALA")]

# Specimen only
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only,
               year == 2022, # Limiting to 2022 for now
               identificationIncorrect == FALSE) %>%  
  galah_select(all_of(IA_fields_split$`1`), group = "assertions") %>%
  atlas_occurrences() -> invert_2022_p1

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022, # Limiting to 2022 for now 
               identificationIncorrect == FALSE) %>%  
  galah_select(all_of(IA_fields_split$`2`)) %>% 
  atlas_occurrences() -> invert_2022_p2

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022, # Limiting to 2022 for now 
               identificationIncorrect == FALSE) %>%  
  galah_select(all_of(IA_fields_split$`3`)) %>% 
  atlas_occurrences() -> invert_2022_p3
```

## Download as an arrow::parquet fie using purrr::map

I've set this is a background job for the download. `Sys.sleep(600)` is for taking a 10 minute break so to not overload the API

```{r}
# Fields that IA want
y <- tibble(name = 
                   c("id",
                     "dataResourceUid",
                     "dataResourceName",
                     "institutionID",
                     "institutionName",
                     "collectionID", 
                     "collectionUid",
                     "collectionName",
                     "contentTypes", 
                     "license", 
                     "taxonConceptID",
                     "raw_scientificName",
                     "raw_vernacularName", 
                     "scientificName", 
                     "vernacularName", 
                     "taxonRank", 
                     "kingdom",
                     "phylum",
                     "class",
                     "order",
                     "family",
                     "genus",
                     "species",
                     "subspecies",
                     "institutionCode",
                     "collectionCode",
                     "locality", 
                     "raw_geodeticDatum", 
                     "raw_decimalLatitude",
                     "raw_decimalLongitude", 
                     "decimalLatitude",
                     "decimalLongitude",
                     "coordinatePrecision",
                     "coordinateUncertaintyInMeters",
                     "country",
                     "stateProvince", 
                     "cl959",
                     "cl21",
                     "cl1048",
                     "minimumElevationInMeters", 
                     "maximumElevationInMeters", 
                     "minimumDepthInMeters",
                     "maximumDepthInMeters",
                     "individualCount",
                     "recordedBy",
                     "eventDate",
                     "year",
                     "month",
                     "verbatimEventDate",
                     "basisOfRecord",
                     "raw_basisOfRecord",
                     "occurrenceStatus",
                     "raw_sex", 
                     "sex",
                     "preparations",
                     "outlierLayer",
                     "spatiallyValid", 
                     "catalogNumber"),
                 type = "field") 

# Set the call as galah_select
attr(y, "call") <- "galah_select"

# Split fields into 3 parts for download
y_ls <- split(y, f = ceiling(seq_along(y$name)/20))

# Add id in 2nd and 3rd list so output can all be joined later by id
y_ls$`2` <- bind_rows(tibble(name = "id",
                             type = "field"),
                      y_ls$`2`)

y_ls$`3` <- bind_rows(tibble(name = "id",
                             type = "field"),
                      y_ls$`3`)

# To get all the assertions
y_ls$`4` <- bind_rows(tibble(name = "id",
                             type = "field"),
                      galah_select(group = "assertions"))

# The rest of the galah_call as a function
get_occ <- function(y){
  
  # Phylums
  afd_taxon_phylums <- c("ANNELIDA",
                         "ARTHROPODA",
                         "BRACHIOPODA",
                         "BRYOZOA",
                         "CHAETOGNATHA",
                         "GASTROTRICHA",
                         "GNATHOSTOMULIDA",
                         "MOLLUSCA",       
                         "NEMATODA",        
                         "NEMATOMORPHA",    
                         "NEMERTEA",       
                         "ONYCHOPHORA",    
                         "PLATYHELMINTHES",
                         "PORIFERA",        
                         "ROTIFERA",        
                         "TARDIGRADA")
  
  # basisOfRecord filters
  specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")
  x <- 
  galah_call() |>
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", 
                 search = FALSE) |>
  galah_filter(basisOfRecord == specimen_only, 
               identificationIncorrect == FALSE)
  
  x$select <- y
  
   x |> 
    atlas_occurrences() |> 
    arrow::write_parquet(sink = paste0("data/galah/IA_", Sys.time()))
   
   Sys.sleep(600)
}

## Test it out for one set of fields
get_occ(y_ls$`4`)

## Background R Studio Job for all lists (errors - can't identify afd_taxon_phylums)
job({
  map(y_ls,
    get_occ)
})

## But works if run in console
map(y_ls,
    get_occ)

```
## Read in parquet and join 3 parts together
```{r}
p1 <- open_dataset("data/galah/IA_2023-01-09 12:24:35")
p2 <- open_dataset("data/galah/IA_2023-01-09 12:41:56")
p3 <- open_dataset("data/galah/IA_2023-01-09 12:57:59")

p1 %>% 
  left_join(p2, by = "recordID") %>% 
  left_join(p3, by = "recordID") %>% 
  write_csv("output/IA_mass_download.csv")
  
  


```


### Join the different parts together

```{r}
# Left join these together
# 2022 version
colnames(invert_2022_p1); ncol(invert_2022_p1)
colnames(invert_2022_p2); ncol(invert_2022_p2)
colnames(invert_2022_p3); ncol(invert_2022_p3)

length(IA_fields)

# 2022 version
left_join(invert_2022_p1, invert_2022_p2, by = "recordID") %>% 
  left_join(., invert_2022_p3, by = "recordID") -> invert_2022

readr::write_csv(invert_2022, "output/invert_2022.csv")
```

### RStudio job of background download 

```{r}
job({
galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only,
               identificationIncorrect == FALSE) %>% 
  galah_select(all_of(IA_fields_split$`1`)) %>%
  atlas_occurrences() -> invert_p1

  saveRDS(invert_p1, "output/invert_p1")

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only,
               identificationIncorrect == FALSE) %>% 
  galah_select(all_of(IA_fields_split$`2`)) %>% 
  atlas_occurrences() -> invert_p2
  
 saveRDS(invert_p2, "output/invert_p2")
  
  galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only,
               identificationIncorrect == FALSE)
  galah_select(all_of(IA_fields_split$`3`)) %>% 
  atlas_occurrences() -> invert_p3
  
   saveRDS(invert_p3, "output/invert_p3")
})
```

### Using Coordinate Cleaner to further clean the records data

Check out https://docs.ropensci.org/CoordinateCleaner/

An really powerful too to remove occurrence records that are in the seas! Not a perfect solution but a great first pass

```{r}
invert_2022 %>% 
  data.frame() %>% 
  clean_coordinates(lon = "decimalLongitude", lat = "decimalLatitude", 
                    species = "scientificName") -> cc_flags

summary(cc_flags)
plot(cc_flags, lon = "decimalLongitude", lat = "decimalLatitude")

# Exclude sea records
cc_flags$.sea

invert_2022_cc <- invert_2022[cc_flags$.sea,]

# Get map of Australia
aus <- st_transform(ozmaps::ozmap_country, 4326)

ggplot() + 
  geom_sf(data = aus, 
          colour = "black", 
          fill = "white")  + 
  geom_point(data = invert_2022_cc, 
             mapping = aes(decimalLongitude, decimalLatitude), 
             colour = "black", size = 0.8, alpha = 0.005) +
  coord_sf(xlim=c(110, 152), 
           ylim=c(-5,-44)) 

```

### Checks of downloads 

No changes made here as a lot of the code below here applies to a list

```{r}

## Check field names across all downnloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
dat_cols <- names(readRDS(ala[1])$data)
for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field names as per specified list = "),
          all(dat_cols==names(f)))
}

all(fields %in% names(f$data)) ## because names are different even if fields correspond
fields[which(fields %!in% names(f$data))]


## Check field classes across all downloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
ala <- names(sort(sapply(ala, file.size)))
f <- readRDS(ala[1])$data
dat_cols <- names(f)
coltypes <- sapply(f[,..dat_cols], class)

for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  f_coltypes <- sapply(f[,..dat_cols], class)
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field classes as per specified list = "),
          all(coltypes==f_coltypes))
}
```

### Some data cleaning steps that are in get_ala_taxondata.R

```{r}
invert_2022_test <- invert_2022
invert_2022_test$decimalLatitude[sample(nrow(invert_2022_test),5)] <-NA # replace 5 obs with NA for a test

# Excluding rows with no coordinate data (test)
invert_2022_test %>% filter(is.na(decimalLatitude) ) %>% nrow() # 5 rows are detected gets excluded

invert_2022_test %>% filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) #nrow = 128

# Excluding rows with no coordinate data
invert_2022 %>% filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) -> invert_2022


# Treating date as date and coordinates as numeric
invert_2022 %>% mutate(verbatimEventDate = lubridate::dmy(verbatimEventDate),
                      decimalLatitude = as.numeric(decimalLatitude),
                      decimalLongitude = as.numeric(decimalLongitude),
                      verbatimLatitude = as.numeric(verbatimLatitude),
                      verbatimLongitude = as.numeric(verbatimLongitude)
                      )
```


### Merge downloaded data

Arthropods is a big data set so its being treated as a `data.table` obj
Looks like counts are reformatted from list to a data frame and the class of each variable is assigned to coltypes

```{r}
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)

## Sort files by size
ala <- names(sort(sapply(ala, file.size), decreasing = TRUE))

## Create data table with Arthropoda dataset (biggest dataset)
f <- readRDS(ala[1])
dat_cols <- names(f$data)
dat_counts <- t(as.data.frame(f$counts))
dat_counts
colnames(dat_counts) <- names(f$counts)
ala_merged <- as.data.table(f$data)
dim(ala_merged)
coltypes <- sapply(ala_merged[,..dat_cols], class)
rm(f)
```

### Merge in remaining datasets

Here the code deals with everything except Arthopods.
By merging I think its just collapsing down the lists of each phylum
Some code to deal with mismatched column classes from different phylum.
I suspect this might not be an issue as `{galah}` downloads in one giant tibble, but requires testing and splitting phyla into lists

```{r}
for (i in 2:length(ala)) {
  f <- ala[i]
  f <- readRDS(f)
  c <- t(as.data.frame(f$counts))
  f <- as.data.table(f$data)
  
  message(cat("Processing dataset ", i, " :", ala[i], " ..."))
  
  dat_counts <- rbind(dat_counts, c)      
  message(cat("Total number of clean records: "),
          sum(dat_counts[,4]))
  
  message(cat("Matching column classes..."))
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  f_mismatch <- which(!(coltypes == f_coltypes))
  for (k in f_mismatch) set(f, j = k, value = eval(parse(text=paste0("as.", coltypes[k], "(f[[k]])"))))
  
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  message(cat("Checking columns classes are same... "),
          all(f_coltypes==coltypes))
  
  message(cat("Merging dataset ..."))
  ala_merged <- rbind(ala_merged, f, use.names = TRUE, fill=TRUE)
  message(cat("Dimensions of merged data: "),
          dim(ala_merged)[1])
  message("\n")
  rm(c,f) 
}

```

### Post merge checks

No changes made here

```{r}
## Check
message(cat("#rows in merged data = sum of cleaned records : "),
        nrow(ala_merged) == sum(dat_counts[,4]))
```

### Save counts and occurences outputs as seperate .rds

```{r}
rownames(dat_counts) <- gsub(".rds", "", basename(ala))
saveRDS(dat_counts, file = file.path(output_dir, "ala_counts.rds"))

saveRDS(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".rds")))
write.csv(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".csv")), row.names = FALSE)

```

#### EXTRA

In next version of `{galah}`, users will be able to obtain a list of assertions. Currently you can view the assertions in messy form [here](https://github.com/AtlasOfLivingAustralia/biocache-store/blob/5c55b021d283026868924f4c3c88a3061ec06956/src/main/scala/au/org/ala/biocache/vocab/AssertionCodes.scala)

```{r}
# ## Explore ALA fields
# ala_fields("occurrence", as_is=TRUE)
# names(ala_fields("occurrence"))
# ala_fields("occurrence")$name
# "assertions" %in% ala_fields("occurrence")$name
# ala_fields()$name[grep("assertions", ala_fields("occurrence")$name)] 
# ala_fields()$description[grep("assertions", ala_fields("occurrence")$name)]

## ALA assertions
ala_fields("assertions",as_is=TRUE)
write.csv(ala_fields("assertions",as_is=TRUE), file = "./output/assertions.csv")
```

