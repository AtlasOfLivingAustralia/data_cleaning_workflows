---
title: "Download ALA by taxon"
author: "Payal Bal, Fonti Kar"
date: "2022-08-30"
output: html_document
editor_options: 
  chunk_output_type: console
---

Below I have split up the `download_ALA_bytaxon` workflow into chunks. I provided a streamlined / `galah`-friendly version directly below the original NESP code.I've also included some notes under each heading and as # comments throughout.

I think the major difference between the NESP bugs and current galah workflow is that NESP bugs workflow supplies a species list from AFD `afd_taxonomy$PHYLUM` to query ALA, whereas `galah` filters by ALL INVERTEBRATES first and then downloads the occurrences. We recommend processing the download and cross match with AFD after the download.

Some of the latter parts of NESP bugs workflow involves collapsing down the lists of download (organised by phylum).  I assumed this was a product for parallel processing using `future` so I didn't write streamlined these sections.  

**Cautionary note: It is advised to NOT to parallelise downloads from `galah`** as this has a very high chance of stalling BioCache API and the user will get kicked out. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
```

### Set working environment 

```{r}
rm(list = ls())

# install.packages("pacmac")
pacman::p_load(data.table, galah, dplyr, sp, raster, future, future.apply)
```

### Setting up server paths

An .Rproj can alleviate some of this path-setting steps but everyone's computer is different. 

```{r}
## Server paths
source(file.path(getwd(),"nesp_bugs", "scripts/get_ala_taxondata.R"))
bugs_dir = "/tempdata/research-cifs/uom_data/nesp_bugs_data"
output_dir = file.path(bugs_dir, "outputs")

# ## Local paths
# source(file.path(getwd(), "scripts/get_ala_taxondata.R"))
# output_dir = "/Volumes/uom_data/nesp_bugs_data/outputs"

ala_dir <- file.path(bugs_dir, "ALA", "download_bytaxon")
if(!dir.exists(ala_dir)) dir.create(ala_dir)
```

### Configuring ALA account
```{r}
galah_config(email = "fonti.kar@gmail.com", 
             atlas = "Australia",
             download_reason_id = 10 # testing reason
             )
```

### Reading in a cleaned AFD species list

```{r}
## Load AFD taxonomic checklist ####
afd_taxonomy <- fread("output/afd_splist_full.csv")
afd_taxon <- unique(afd_taxonomy$PHYLUM) 
length(afd_taxon)
```

### Count records and write as .csv 

`taxon.counts()` calls for `get_ala_taxondata.R` to get a tally of each taxon, but `get_ala_taxondata.R` actually has download occurrence records first, cleans it up a bit and then tallies. More notes below:

`get_ala_taxondata.R` downloads occurrence records depending on: 

1. basis of record if `specimens_only = TRUE`
    e.g. 
    "PreservedSpecimen", "LivingSpecimen", 
    "MachineObservation", "EnvironmentalDNA","GenomicDNA"
2. [fields](https://biocache.ala.org.au/ws/index/fields), this version is better for the eyes: https://biocache.ala.org.au/fields
3. [Darwin Core fields](https://biocache.ala.org.au/ws/index/fields) via ALA4R::occurrences::`qa` argument. 
   `qa` was preferred over `extra = "assertions"` because `extra = "assertions"` only lists one assertion, even if the record has multiple.
    e.g.
    "el1055", "el1056", #endemism 
    "el682", #migratory
    "disposition", 
    "assertions","assertions_unchecked", 
    "raw_data_generalizations",
    "duplicate_record", "duplicate_status", 
    "sensitive","taxonomic_issue")

Some data cleaning steps in the `get_ala_taxondata.R`: 
1. Removes rows where `latitude` and `latitude` are NA
```{r}
taxon.counts <- lapply(afd_taxon,
                       function(x){
                         tmp <- tryCatch(expr = get_ala_taxondata(x,
                                                                  get_counts_only = TRUE,
                                                                  specimens_only = TRUE,
                                                                  dst = ala_dir),
                                         error = function(e){ 
                                           print(paste("\nNot run: no records for", x))
                                           
                                           cat(paste(x, "\n"),
                                               file = nodatalog, 
                                               append = T)
                                         })
                       })

taxon.counts <- as.data.frame(taxon.counts)
colnames(taxon.counts) <- afd_taxon
write.csv(taxon.counts, file.path(output_dir, "taxon_counts.csv"))
```

### {galah} for getting counts for each Invertebrate Phylum

Fields for [`basisOfRecord`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/basisOfRecord)

**Note:** ALA has collapsed `EnvironmentalDNA`, `GenomicDNA`" into `Material_Sample` 
ALA has added [`contentTypes`](https://github.com/AtlasOfLivingAustralia/ala-dataquality/wiki/contentTypes) to further delineate these categories

```{r}
# Equvialent to n.all 
# Obtain counts for each phylum
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier # Filters NOT Chordata
  ) %>% 
  galah_group_by(phylum) %>%
  atlas_counts() -> n.all 

# To view more rows
n.all %>% print(n = 50) # Change n for view more

# This is equivalent to n.obs
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier,
    basisOfRecord == "HUMAN_OBSERVATION" # Filters to human observed
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() -> n.obs

n.obs %>% print(n = 50)

# This is equivalent to n.spec
# If need to filter basis of record, equivalent to specimen_only = TRUE in `get_ala_taxondata`
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")
galah_call() %>%
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only # Filters to specimen only
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() -> n.spec

# Format data to save as a .csv
# Rename
n.all %>% rename(n.all = count) -> n.all
n.obs %>% rename(n.obs = count) -> n.obs
n.spec %>% rename(n.spec = count) -> n.spec

# Join all counts
left_join(n.all, n.obs) %>% 
  left_join(., n.spec) -> taxon_counts

# Save as csv
write.csv(taxon.counts, "output/taxon_counts.csv")
```

### {galah} for filtering records by assertions

Assertions are used to filter down records ensure the data is fit for use. 

To display all assertion fields use:

```{r}
show_all("assertions") %>% print(n = 250)
```

Users can filter records using `galah_filter()` as below.  
Replacing `UNKNOWN_KINGDOM` with whatever assertion name youâ€™d like to use. 
Alternatively, to exclude by multiple assertions, use `c()` 

This method of filtering can also applied to downloading records as well as obtaining counts.

```{r}
# For single assertion exclusions
galah_call()  %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    UNKNOWN_KINGDOM == FALSE
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts() 

# For exclusions using multiple assertions
IA_assertions <- c("UNKNOWN_KINGDOM", "TAXON_MATCH_NONE",
                   "COORDINATE_PRECISION_MISMATCH", "MISSING_GEODETICDATUM")

galah_call() %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only, # Filters to specimen only
    assertions != IA_assertions
  ) %>% 
  galah_group_by(phylum) %>% 
  atlas_counts()
```

### {galah} download occurrence records

Just searching each field from the `fields` vector above here https://biocache.ala.org.au/fields to make a new vector that works with the current API system. **Note: to avoid parallel process this step to avoid crashes to API system**

Unable to find equivalent fields for: 

- `taxonomic_kosher`

Some fields did not have an exact match but I took my best guess to find a match i.e. original ~ best guess

-raw_taxon_name ~ raw_scientificName
-raw_datum ~ raw_geodeticDatum
-taxon_name ~ scientificName
-common_name ~ vernacularName
-rank ~ taxonRank
-state ~ locality
-min_elevation_d not sure what _d represents ~ minimumElevationInMeters
-max_elevation_d not sure what _d represents ~ maximumElevationInMeters
-min_depth_d not sure what _d represents  ~  minimumDepthInMeters
-max_depth_d not sure what _d represents ~ maximumDepthInMeters
-collector ~ recordedBy
-occurrence_date ~ eventDate
-geospatial_kosher ~ spatiallyValid

#### Investigate fields are there

```{r}
ALA_fields <- show_all(fields)

ALA_fields %>% print(n = 100)

# Search the fields
search_all(fields, "coordinatePrecision") %>% print(n = 50) 

# Fields that IA want
IA_fields<- c("id","dataResourceUid","dataResourceName",
              "institutionID","institutionName",
              "collectionID", "collectionUid","collectionName",
              "contentTypes", 
              "license", 
              "taxonConceptID",
              "raw_scientificName" ,"raw_vernacularName", 
              "scientificName", 
              "vernacularName", 
              "taxonRank", 
              "kingdom","phylum","class","order", 
              "family","genus","species","subspecies",
              "institutionCode","collectionCode",
              "locality", 
              "raw_geodeticDatum", 
              "raw_decimalLatitude","raw_decimalLongitude", 
              "decimalLatitude","decimalLongitude",
              "coordinatePrecision","coordinateUncertaintyInMeters",
              "country","stateProvince", 
              "cl959","cl21","cl1048",
              "minimumElevationInMeters", "maximumElevationInMeters", 
              "minimumDepthInMeters", "maximumDepthInMeters",
              "individualCount",
              "recordedBy",
              "eventDate",
              "year","month",
              "verbatimEventDate",
              "basisOfRecord","raw_basisOfRecord",
              "occurrenceStatus",
              "raw_sex", "sex",
              "preparations",
              "outlierLayer",
              "spatiallyValid", "catalogNumber", "decimalLatitude","decimalLongitude",
              "coordinatePrecision","coordinateUncertaintyInMeters",
              "country","stateProvince", 
              "cl959","cl21","cl1048",
              "minimumElevationInMeters", "maximumElevationInMeters", 
              "minimumDepthInMeters", "maximumDepthInMeters",
              "individualCount",
              "recordedBy",
              "eventDate",
              "year","month",
              "verbatimEventDate",
              "basisOfRecord","raw_basisOfRecord",
              "occurrenceStatus",
              "raw_sex", "sex",
              "preparations",
              "outlierLayer",
              "spatiallyValid", "catalogNumber") 

length(IA_fields)

# Split into 4 parts
IA_fields_split <- split(IA_fields, ceiling(seq_along(IA_fields)/25))

# We want to eventually join by recordID so need to add this to the 2-4 parts
IA_fields_split$`2` <- c("id", IA_fields_split$`2`)
IA_fields_split$`3` <- c("id", IA_fields_split$`3`)
IA_fields_split$`4` <- c("id", IA_fields_split$`4`)
```


```{r}
specimen_only <- c("PRESERVED_SPECIMEN", "LIVING_SPECIMEN", 
                   "MACHINE_OBSERVATION", "MATERIAL_SAMPLE")

# Limiting my download to a specific year for testing purposes
galah_call() %>% 
  galah_filter(
    year == 2022,
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, 
    basisOfRecord == specimen_only # Filters to specimen only
  ) %>% 
  galah_select(fields, group = "assertions") %>% 
  atlas_occurrences() 

# saveRDS(invert_2022, "output/invert_2022.rds")
invert_2022 <- readRDS("output/invert_2022.rds")

## Doing the mass download

galah_call() %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, # Filters NOT Chordata
    basisOfRecord == specimen_only # Filters to specimen only
  ) %>% 
  galah_select(fields, group = "assertions") %>%  
  atlas_occurrences() -> invert_all

# saveRDS(invert_all, "output/invert_all.rds")
invert_all <- readRDS("output/invert_all.rds")
```

### {galah} download: user supplied list

I realised there are some inverts that are in Chordata, so we definitely don't want to exclude all Chordates

```{r}
search_taxa(afd_taxon) # Issues with ACANTHOCEPHALA its technically a Phylum but recorded as a class in here https://bie.ala.org.au/species/NZOR-6-53977

# Currently working with identifier 
galah_call() %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  atlas_counts()

# Current workaround
afd_taxon_phylums <- afd_taxon[-which(afd_taxon == "ACANTHOCEPHALA")]
      
galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022) %>%  # Limiting to 2022 for now
  galah_select(glue::as_glue(IA_fields_split$`1`)) %>%
  atlas_occurrences() -> invert_2022_p1

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022) %>%  # Limiting to 2022 for now 
  galah_select(glue::as_glue(IA_fields_split$`2`)) %>% 
  atlas_occurrences() -> invert_2022_p2

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022) %>%  # Limiting to 2022 for now 
  galah_select(glue::as_glue(IA_fields_split$`3`)) %>% 
  atlas_occurrences() -> invert_2022_p3

galah_call() %>% 
  galah_identify(afd_taxon_phylums) %>% 
  galah_identify("https://biodiversity.org.au/afd/taxa/3cbb537e-ab39-4d85-864e-76cd6b6d6572", search = FALSE) %>% 
  galah_filter(basisOfRecord == specimen_only, 
               year == 2022) %>%  # Limiting to 2022 for now 
  galah_select(glue::as_glue(IA_fields_split$`4`)) %>% 
  atlas_occurrences() -> invert_2022_p4

# Left join
colnames(invert_2022_p1)
colnames(invert_2022_p2)
colnames(invert_2022_p3)
colnames(invert_2022_p4)
  
```

Benchmarking the download time to compare with ALA4R parallel method

```{r}
# devtools::install_github("jabiru/tictoc")
library(tictoc)

tic("galah download")
galah_call() %>% 
  galah_filter(
    taxonConceptID == galah_identify("Animalia")$identifier, 
    taxonConceptID != galah_identify("Chordata")$identifier, # Filters NOT Chordata
    basisOfRecord == specimen_only # Filters to specimen only
  ) %>% 
  galah_select(fields) %>%  
  atlas_occurrences() -> invert_all
toc()

# galah download: 1949.915 sec elapsed
1949.915/60 # 32 mins
```

### Checks of downloads 

No changes made here as a lot of the code below here applies to a list

```{r}

## Check field names across all downnloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
dat_cols <- names(readRDS(ala[1])$data)
for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field names as per specified list = "),
          all(dat_cols==names(f)))
}

all(fields %in% names(f$data)) ## because names are different even if fields correspond
fields[which(fields %!in% names(f$data))]


## Check field classes across all downloads
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)
ala <- names(sort(sapply(ala, file.size)))
f <- readRDS(ala[1])$data
dat_cols <- names(f)
coltypes <- sapply(f[,..dat_cols], class)

for (i in 2:length(ala)) {
  f <- readRDS(ala[i])$data
  f_coltypes <- sapply(f[,..dat_cols], class)
  message(cat("Checking dataset ", i, " :", ala[i], " ...\n"),
          cat("field classes as per specified list = "),
          all(coltypes==f_coltypes))
}
```

### Some data cleaning steps that are in get_ala_taxondata.R

```{r}
invert_2022_test <- invert_2022
invert_2022_test$decimalLatitude[sample(nrow(invert_2022_test),5)] <-NA # replace 5 obs with NA for a test

# Excluding rows with no coordinate data (test)
invert_2022_test %>% filter(is.na(decimalLatitude) ) %>% nrow() # 5 rows are detected gets excluded

invert_2022_test %>% filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) #nrow = 128

# Excluding rows with no coordinate data
invert_2022 %>% filter(! is.na(decimalLatitude) & ! is.na(decimalLongitude) &
                             ! is.na(verbatimLatitude) & ! is.na(verbatimLongitude)) -> invert_2022


# Treating date as date and coordinates as numeric
invert_2022 %>% mutate(verbatimEventDate = lubridate::dmy(verbatimEventDate),
                      decimalLatitude = as.numeric(decimalLatitude),
                      decimalLongitude = as.numeric(decimalLongitude),
                      verbatimLatitude = as.numeric(verbatimLatitude),
                      verbatimLongitude = as.numeric(verbatimLongitude)
                      )
```


### Merge downloaded data

Arthropods is a big data set so its being treated as a `data.table` obj
Looks like counts are reformatted from list to a data frame and the class of each variable is assigned to coltypes

```{r}
ala <- list.files(file.path(ala_dir), include.dirs = FALSE, full.names = TRUE)

## Sort files by size
ala <- names(sort(sapply(ala, file.size), decreasing = TRUE))

## Create data table with Arthropoda dataset (biggest dataset)
f <- readRDS(ala[1])
dat_cols <- names(f$data)
dat_counts <- t(as.data.frame(f$counts))
dat_counts
colnames(dat_counts) <- names(f$counts)
ala_merged <- as.data.table(f$data)
dim(ala_merged)
coltypes <- sapply(ala_merged[,..dat_cols], class)
rm(f)
```

### Merge in remaining datasets

Here the code deals with everything except Arthopods.
By merging I think its just collapsing down the lists of each phylum
Some code to deal with mismatched column classes from different phylum.
I suspect this might not be an issue as `{galah}` downloads in one giant tibble, but requires testing and splitting phyla into lists

```{r}
for (i in 2:length(ala)) {
  f <- ala[i]
  f <- readRDS(f)
  c <- t(as.data.frame(f$counts))
  f <- as.data.table(f$data)
  
  message(cat("Processing dataset ", i, " :", ala[i], " ..."))
  
  dat_counts <- rbind(dat_counts, c)      
  message(cat("Total number of clean records: "),
          sum(dat_counts[,4]))
  
  message(cat("Matching column classes..."))
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  f_mismatch <- which(!(coltypes == f_coltypes))
  for (k in f_mismatch) set(f, j = k, value = eval(parse(text=paste0("as.", coltypes[k], "(f[[k]])"))))
  
  f_coltypes <- as.character(sapply(f[,..dat_cols], class))
  message(cat("Checking columns classes are same... "),
          all(f_coltypes==coltypes))
  
  message(cat("Merging dataset ..."))
  ala_merged <- rbind(ala_merged, f, use.names = TRUE, fill=TRUE)
  message(cat("Dimensions of merged data: "),
          dim(ala_merged)[1])
  message("\n")
  rm(c,f) 
}

```

### Post merge checks

No changes made here

```{r}
## Check
message(cat("#rows in merged data = sum of cleaned records : "),
        nrow(ala_merged) == sum(dat_counts[,4]))
```

### Save counts and occurences outputs as seperate .rds

```{r}
rownames(dat_counts) <- gsub(".rds", "", basename(ala))
saveRDS(dat_counts, file = file.path(output_dir, "ala_counts.rds"))

saveRDS(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".rds")))
write.csv(ala_merged, file = file.path(output_dir, paste0("merged_ala_", Sys.Date(),".csv")), row.names = FALSE)

```

#### EXTRA

In next version of `{galah}`, users will be able to obtain a list of assertions. Currently you can view the assertions in messy form [here](https://github.com/AtlasOfLivingAustralia/biocache-store/blob/5c55b021d283026868924f4c3c88a3061ec06956/src/main/scala/au/org/ala/biocache/vocab/AssertionCodes.scala)

```{r}
# ## Explore ALA fields
# ala_fields("occurrence", as_is=TRUE)
# names(ala_fields("occurrence"))
# ala_fields("occurrence")$name
# "assertions" %in% ala_fields("occurrence")$name
# ala_fields()$name[grep("assertions", ala_fields("occurrence")$name)] 
# ala_fields()$description[grep("assertions", ala_fields("occurrence")$name)]

## ALA assertions
ala_fields("assertions",as_is=TRUE)
write.csv(ala_fields("assertions",as_is=TRUE), file = "./output/assertions.csv")
```

