[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ALA Data Cleaning",
    "section": "",
    "text": "This is a book created by the Science and Decision Support team at the Atlas of Living Australia (ALA) to support researchers and decision makers on a consistent process of cleaning up biodiversity datasets, obtained or not from ALA.\nThis is a free and live document. If you have questions or suggestions please reach us at (e-mail)."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "2  Preparation",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html#importance-of-data-cleaning",
    "href": "intro.html#importance-of-data-cleaning",
    "title": "1  Introduction",
    "section": "1.1 Importance of data cleaning",
    "text": "1.1 Importance of data cleaning\nData cleaning is a process of identifying, fixing or/and removing incorrect, doubtful, mislabeled, or incomplete data within a dataset. It is a critical step in any research on biodiversity, as it guarantees the most trustful data to perform analyses for research. The anecdote “garbage in, garbage out” reflects this idea - a study analysis is as good as the data used.\nData problems can come from many sources, from the precision of the coordinate points during collection in the field to the combination of multiple data sources, where mismatches can happen. For this reason, the data cleaning process will vary from different datasets, making it hard to point out a “work for all” solution.\nAlthough data cleaning can be time-consuming, an established and consistent process will result in quality data to be used in future analyses."
  },
  {
    "objectID": "intro.html#components",
    "href": "intro.html#components",
    "title": "1  Introduction",
    "section": "1.2 Components",
    "text": "1.2 Components\nList here the components of data cleaning."
  },
  {
    "objectID": "intro.html#limitations",
    "href": "intro.html#limitations",
    "title": "1  Introduction",
    "section": "1.4 Limitations",
    "text": "1.4 Limitations\n\nType and quality of the information available\nDifferent types of research require different data cleaning steps\netc etc"
  },
  {
    "objectID": "intro.html#how-to-use-this-book",
    "href": "intro.html#how-to-use-this-book",
    "title": "1  Introduction",
    "section": "1.5 How to use this book",
    "text": "1.5 How to use this book\nList here\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "preparation.html#determine-a-naming-authority",
    "href": "preparation.html#determine-a-naming-authority",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.1 Determine a naming authority",
    "text": "2.1 Determine a naming authority\nObtaining distribution data can be done in two ways. One is when the study is focused on a specific species or community. In this case, the search is performed using the scientific and common name of the species or group of species. Another is when there is interest in obtaining a list of all species present in a given location. In this case, the region name or area boundaries can be used to delimit the area of interest.\nRegardless of the study focus, a knowledge of taxonomy is essential to verify the data obtained. For this, different organizations provide updated lists of species or even details of history and taxonomic changes.\nIn Australia, the Australian Plant Name Index (APNI) is the primary naming authority for plants. At the same time, the Australian Faunal Directory (AFD) is the main taxonomic catalog for animal species, providing a list of accepted and authoritative names as a template.\nYet, it is fundamental to understand the taxonomic history of the interest species. Changes in taxonomy, such as species split, new higher level classification (as genera), or species descriptions, highlight the importance of keeping up-to-date with taxonomic literature. This can be achieved via specialized books and scientific papers. Most taxonomic group societies released annual updates on taxonomy. The Atlas of Living Australia (ALA) can also be used to investigate taxonomic changes.\n\n2.1.1 Australian taxonomic group societies\nVERTEBRATES\n\nAmphibians and reptiles - Australian Herpetological Society\n\nBirds - Birdlife Australia\n\nFish - Australian Society for Fish Biology\n\nMammals - The Australian Mammal Society\n\nINVERTEBRATES\n\nArachnology - Australasian Arachnological Society\n\nEntomology - Australian Entomological Society\n\nMalacology - The Malacological Society of Australasia\n\nNematology - Australasian Association of Nematologists"
  },
  {
    "objectID": "preparation.html#download-occurence-data",
    "href": "preparation.html#download-occurence-data",
    "title": "2  Step 1. Download and data preparation",
    "section": "2.2 Download occurence data",
    "text": "2.2 Download occurence data\nThere are many ways to obtain occurrence data of the species or region of interest. A literature search using web search engineers as Google Scholar. Data can also be obtained directly from data infrastructure websites as the Global Biodiversity Information Facility (GBIF) or from citizen science initiatives, as iNaturalist. Australia counts with ALA, an open access Australia’s biodiversity database, that aggregates data from a broad range of projects and initiatives, museum and universities, and data uploaded directly into the platform.\nAn alternative to download data directly from project websites is to use R packages created specifically for data occurence download (see list on chapter xx or below). The ALA has developed an R package exclusively to obtain data from the Atlas. Galah enables users to locate and download species occurrence records (observations, specimens, eDNA records, etc.), taxonomic information, or associated media such as images or sounds, and to restrict their queries to particular taxa or locations."
  },
  {
    "objectID": "preparation.html#merge-datasets",
    "href": "preparation.html#merge-datasets",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.3 Merge datasets",
    "text": "2.3 Merge datasets\nIf the data was obtained from multiple places, it is important to standardize the data fields and merge the data carefully."
  },
  {
    "objectID": "preparation.html#familiarise-yourself-with-your-data",
    "href": "preparation.html#familiarise-yourself-with-your-data",
    "title": "2  Step 1. Download and data preparation",
    "section": "2.4 Familiarise yourself with your data",
    "text": "2.4 Familiarise yourself with your data\nFamiliarise yourself with the distribution of your data. Before you get started on any data cleaning it can help to use exploraoty visuals (like plotting the data on a map) to see what it looks like. Other tools can also help for example if you’re know your dat set should only have positive values. would add an “extra step” here to fix obvious coordinate errors (sometimes the corrdinateds are just inverted)."
  },
  {
    "objectID": "preparation.html#check-your-metadata",
    "href": "preparation.html#check-your-metadata",
    "title": "2  Step 1. Download and data preparation",
    "section": "2.5 Check your metadata",
    "text": "2.5 Check your metadata\nUnderstanding your metadata provides context to the structure of the data, and how the data was collected etc. this includes check coordinate errors as missing a negative"
  },
  {
    "objectID": "preparation.html#first-look-clean-up",
    "href": "preparation.html#first-look-clean-up",
    "title": "2  Step 1. Download and prepare your data",
    "section": "2.6 First-look clean-up",
    "text": "2.6 First-look clean-up\nRemove records from doubtful sources. Sometimes the source iof the data is questionable and it’s better to discard it straight form the beginning: from drawings, photographs or multimedia objects."
  },
  {
    "objectID": "preparation.html#ala-specific",
    "href": "preparation.html#ala-specific",
    "title": "2  Step 1. Download and data preparation",
    "section": "2.8 ALA specific",
    "text": "2.8 ALA specific\nFilter by basis of record. This can be useful when downloading data or after to make sure you are only trying to clean and deal with the type of records you want. It can ensure consistency when getting data frommultiple places. Examples of basis of record filters include: exlcuding citisen science data, only including herbarium data etc"
  },
  {
    "objectID": "preparation.html#extra-r-packages-that-can-help-on-data-preparation",
    "href": "preparation.html#extra-r-packages-that-can-help-on-data-preparation",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.8 EXTRA: R packages that can help on data preparation?",
    "text": "2.8 EXTRA: R packages that can help on data preparation?"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "ALA Data Cleaning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWrite names here"
  },
  {
    "objectID": "cleaning.html#text-here",
    "href": "cleaning.html#text-here",
    "title": "3  Step 2. Cleaning and correcting your data",
    "section": "3.1 Text here",
    "text": "3.1 Text here\nhere"
  },
  {
    "objectID": "improving.html#text-here",
    "href": "improving.html#text-here",
    "title": "4  Step 3. Data Enrichment",
    "section": "4.1 Text here",
    "text": "4.1 Text here\nhere"
  },
  {
    "objectID": "cleaning.html#taxonomy",
    "href": "cleaning.html#taxonomy",
    "title": "3  Step 2. Data cleaning and standardization",
    "section": "3.1 Taxonomy",
    "text": "3.1 Taxonomy\nThe following steps assume that the taxonomic history of the included species is known and correct and out-of-date names and synonymy have been fixed during the data preparation.\n\n3.1.1 Standarize scientific names\nEnsure that the spelling is correct and fix errors as they appear. It is essential to decide on a standard way to record the scientific names and be consistent. For example, it can always start with upper case and separate the names with an underline, as in “Moloch_horridus”. It is also important to check if high hierarchy taxonomic classification is correct and not missed, as family and order.\n\n\n3.1.2 Taxonomic cleaning\n\nRemove non-identifiable specimens: If a species is not discernible with checks of spelling and taxonomy, then it should be removed.\nRemove records with insufficient taxon identification: If a record is not identified down to the species level and this level of detail is needed for the study, then the record should be removed.\nRemove records with miss-identified taxonomy: Incorrectly identified species, or unrecognized species names compared to a naming authority, should be removed.\n\n\n\n3.1.3 Optional taxonomic cleaning\nThese steps can be applicable depending on the research aim.\n\nRemove non-native species: This step is a common requirement. A list can be obtained from the Global Register of Introduced and Invasive Species (GRIIS)\nRemove domesticated specimens.\nRemove extinct species.\nRemove specific taxa/life stages depending on the study: For example, if working with terrestrial data, it is necessary to remove marine taxa."
  },
  {
    "objectID": "cleaning.html#distribution-data",
    "href": "cleaning.html#distribution-data",
    "title": "3  Step 2. Cleaning and correcting your data",
    "section": "3.2 Distribution data",
    "text": "3.2 Distribution data\nhere"
  },
  {
    "objectID": "cleaning.html#ecological-data",
    "href": "cleaning.html#ecological-data",
    "title": "3  Step 2. Cleaning and correcting your data",
    "section": "3.3 Ecological data",
    "text": "3.3 Ecological data\nhere"
  },
  {
    "objectID": "improving.html#text-2",
    "href": "improving.html#text-2",
    "title": "4  Step 3. Data Enrichment",
    "section": "4.2 Text 2",
    "text": "4.2 Text 2\nhere"
  },
  {
    "objectID": "cleaning.html#distribution",
    "href": "cleaning.html#distribution",
    "title": "3  Step 2. Data cleaning and standardization",
    "section": "3.2 Distribution",
    "text": "3.2 Distribution\nNow that the taxonomic data is cleaned, it is important to plot your data on a map again and clean up the distribution data.\n\n3.2.1 Coordinates check\nMany coordinates issues can be solved with data manipulation instead of discarding the data immediately. Here are some examples:\n\nCorrect flipped coordinates: These are often data entry errors. Data visualizations can help to detect flipped coordinates. Flipped coordinates typically appear as a clustering of points, whereby swapping the latitude and longitude will place the coordinates where they are expected.\nCorrect poorly formatted coordinates: These are often data entry errors. Different datasets can provide coordinates in different formats, so it is crucial to correct the format of the coordinates before discarding any points.\nCorrect numerical sign confusion: These are often data entry errors. As with flipped coordinates, if there is a clustering of points mirrored to another hemisphere, consider swapping the sign and correct rather than discard the points.\nCorrect/check for records with coordinates in a different country than is listed in the country field: The coordinates could be wrong or just the country listed. Note that the country should only be changed to match the coordinates if the data is trustworthy.\nCorrect/check for records with locality but no occurrence/georeference (if possible): If you have a good locality, consider georeferencing for coordinates rather than discarding the entry. This will likely yield lower-quality data points.\n\nAfter completing the manipulation of the coordinates, if there are still errors, consider removing the data entry:\n\nRemove records with null or missing coordinates that could not be georeferenced: These are often data entry errors. It can be records missing partial or complete information. Maintaining missing values can cause errors.\nRemove records with zero coordinates that could not be georeferenced: These are often data entry errors. When plotting it on a map, zero coordinates will be found around the point at zero latitudes and longitudes. These records will not accurately represent their valid location and must be removed.\nRemove records plotted away from the known species distribution area: These are often data entry errors. They are easily identifiable as outliers. It is essential to check the metadata to ensure that it is a data entry error and not a true outlier. There are several ways of dealing with this issue, but one option can be to mask data to remove points from falling off a determined area.\nRemove records with low coordinate precision: Coordinate precision is vital for the accuracy of models. However, different studies may have different precision cut-offs. For example, coordinate precision below 100km represents the grain size of many macroecological analyses. Some studies have used a cut-off of spatial resolution >25,000m or precision with less than three decimal places (add a reference here). It is important to note that rasterized collections often have a significant proportion of records that might have low coordinate precision.\nRemove records with coordinates assigned to country and province centroids: There are many examples of the origin of centroid data, such as Centre of Country, botanic gardens, zoos, country capitals, biodiversity institutions, urban areas, and gbif headquarters. This issue can happen for several reasons and sometimes can be tricky to spot. Exploratory visuals can help support findings, making it easier to spot clusterings of points. Centroids are common when records are being assigned from georeferencing based on vague locality descriptions or from incorrect georeferencing. Sometimes, records are erroneously entered with the physical location of the specimen or because they represent individuals from captivity or horticulture, which were not clearly labeled as such. In a few cases, zoos and botanic gardens might be where the record was sighted. However, in this case, it is not naturally occurring and should be removed. Records in urban areas may not want to be removed by everyone, but it is essential to note that it could be old data or have vague locality descriptions.\nRemove records outside of the country of interest: In some cases, records outside the country of origin may be outliers. In other cases, they may be perfectly valid. It is important to analyze case-by-case and remove the record if necessary.\nRemove records where longitude and latitude are equal: These are often data entry errors. High likelihood that this is not where the record was recorded and, therefore, should be removed.\nRemove records with faulty coordinates (outside normal range): These are often data entry errors. High likelihood that this is not where the record was recorded and, therefore, should be removed."
  },
  {
    "objectID": "cleaning.html#ecology",
    "href": "cleaning.html#ecology",
    "title": "3  Step 2. Data cleaning and standardization",
    "section": "3.3 Ecology",
    "text": "3.3 Ecology\nIf information on the ecological requirements of the species is available and important for the study, it is important to spend some time checking, standarzing and correcting/removing data as needed. Examples include morphological measurements, that should be in the same metric, information regarding life-stage, and habitat preference."
  },
  {
    "objectID": "intro.html#expect-result",
    "href": "intro.html#expect-result",
    "title": "1  Introduction",
    "section": "1.3 Expect result",
    "text": "1.3 Expect result\nA data that is: - accurate - comprehensive - consistent - relevant - uniform"
  },
  {
    "objectID": "preparation.html#initial-clean-up",
    "href": "preparation.html#initial-clean-up",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.5 Initial clean-up",
    "text": "2.5 Initial clean-up\nAlthough systematic data cleaning is required, some apparent mistakes observed during the initial visualization of the data and metadata can be fixed straight away. It includes removing records from unlikely sources, removing unnecessary details for the study, and fixing minor coordinates errors, such as inverted coordinates."
  },
  {
    "objectID": "preparation.html#rebuild-missing-data",
    "href": "preparation.html#rebuild-missing-data",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.6 Rebuild missing data??",
    "text": "2.6 Rebuild missing data??"
  },
  {
    "objectID": "intro.html#characteristics-of-a-quality-data",
    "href": "intro.html#characteristics-of-a-quality-data",
    "title": "1  Introduction",
    "section": "1.2 Characteristics of a quality data",
    "text": "1.2 Characteristics of a quality data\n\nAccurate. describe here\nComprehensive. describe here\nConsistent. describe here\nRelevant. describe here\nUniform. describe here\n\nfrom (https://www.iteratorshq.com/blog/data-cleaning-in-5-easy-steps/)"
  },
  {
    "objectID": "intro.html#data-cleaning-process",
    "href": "intro.html#data-cleaning-process",
    "title": "1  Introduction",
    "section": "1.3 Data cleaning process",
    "text": "1.3 Data cleaning process\nIn the following chapters, we will dive deep into the different steps of the data-cleaning process. It can be separated into three main steps: Data preparation, standardization, and enrichment (Figure 1).\n\n\n\nFigure 1. Data cleaning process"
  },
  {
    "objectID": "intro.html#importance-of-data-cceaning",
    "href": "intro.html#importance-of-data-cceaning",
    "title": "1  Introduction",
    "section": "1.1 Importance of data Cceaning",
    "text": "1.1 Importance of data Cceaning\nData cleaning is a process of identifying, fixing or/and removing incorrect, doubtful, mislabeled, or incomplete data within a dataset. It is a critical step in any research on biodiversity, as it guarantees the most trustful data to perform analyses for research. The anecdote “garbage in, garbage out” reflects this idea - a study analysis is as good as the data used.\nData problems can come from many sources, from the precision of the coordinate points during collection in the field to the combination of multiple data sources, where mismatches can happen. For this reason, the data cleaning process will vary from different datasets, making it hard to point out a “work for all” solution.\nAlthough data cleaning can be time-consuming, an established and consistent process will result in quality data to be used in future analyses."
  },
  {
    "objectID": "preparation.html#download-occurrence-data",
    "href": "preparation.html#download-occurrence-data",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.2 Download occurrence data",
    "text": "2.2 Download occurrence data\nThere are many ways to obtain species or region of interest occurrence data. A literature search using web search engineers such as Google Scholar. Data can also be obtained directly from data infrastructure websites such as the Global Biodiversity Information Facility (GBIF) or citizen science initiatives, such as iNaturalist. Australia counts on ALA, an open-access Australia’s biodiversity database that aggregates data from a broad range of projects and initiatives, museums and universities, and data uploaded directly into the platform.\nAn alternative to downloading data directly from project websites is to use R packages explicitly created for data occurrence download (see a list in chapter xx or below). The ALA has developed an R package exclusively to obtain data from ALA. Galah enables users to locate and download species occurrence records (observations, specimens, eDNA records, etc.), taxonomic information, or associated media such as images or sounds, and to restrict their queries to particular taxa or locations."
  },
  {
    "objectID": "preparation.html#familiarise-with-the-data-and-metadata",
    "href": "preparation.html#familiarise-with-the-data-and-metadata",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.4 Familiarise with the data and metadata",
    "text": "2.4 Familiarise with the data and metadata\nBefore getting started with data cleaning, a visual inspection of the entire dataset can save time and solve easy-to-spot errors. It can improve the knowledge of the available data and can be done with it. Understanding the metadata provides context to the structure of the data and information on how the data was collected.\nA plot of the occurrence data on a map can follow it. This simple step can highlight coordinates with missing data or even inverted or erroneous coordinates (as points floating in the ocean while focusing on terrestrial species and vice versa).\nOther tools, such as looking for only positive values if that is what is expected, can also be used in this initial data familiarization step."
  },
  {
    "objectID": "preparation.html#optional-steps-on-data-preparation",
    "href": "preparation.html#optional-steps-on-data-preparation",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.7 Optional steps on data preparation",
    "text": "2.7 Optional steps on data preparation\nWhen handling data from the ALA, some filters can help obtain only the type of records required for the study. Filter by “basis of record”, for example, can ensure consistency when getting data from multiple places. For example, it is possible to exclude data originating from citizen science projects or only include data from herbariums and natural history museums."
  },
  {
    "objectID": "cleaning.html#taxonomic-cleaning",
    "href": "cleaning.html#taxonomic-cleaning",
    "title": "3  Step 2. Data standarization",
    "section": "3.2 Taxonomic cleaning",
    "text": "3.2 Taxonomic cleaning\n\nRemove non-identifiable specimens: If a species is not discernible with checks of spelling etc then it should be removed\nRemove records with insufficient taxon identification: If a record is not identified down to the species level and this level of detail is needed then the record should be removed\nRemove records with mis-identified taxonomy: Incorrectly identified species, or unrecognised species name compared to a naming authority\n\n\n3.2.1 Optional taxonomic cleaning\nThis steps can be applicable depending on your research aim.\n\nRemove non-native species: This will be dependant on the type of study you’re conducting, however it’s a common requirement- this GRIIS list can be used for this in Australia\nRemove domesticated specimens\nRemove extinct species\nRemove specific taxa/lifestages depending on study: for example marine taxa or non-vascualr plants. Marine taxa can be difficult as at the ALA for example we do not have a way of discerning marine species so this is often tedious-especially when looking at all life stages"
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "5  Work Example",
    "section": "",
    "text": "Include here an workable example (maybe using galah?)"
  },
  {
    "objectID": "cleaning.html#checklist-of-data-standarization",
    "href": "cleaning.html#checklist-of-data-standarization",
    "title": "3  Step 2. Data cleaning and standardization",
    "section": "3.4 Checklist of data standarization",
    "text": "3.4 Checklist of data standarization\nAdd here a downloadable checklist of issues to be checked"
  },
  {
    "objectID": "improving.html",
    "href": "improving.html",
    "title": "4  Step 3. Data Enrichment",
    "section": "",
    "text": "Now that the data is cleaned and standardized, extra work can be done to guarantee the data quality. These steps will depend on the kind of study being conducted.\n\nBias correction: Account for sampling bias.\nScrutinise outliers: Outliers can be true outliers or data errors. True outliers are not necessarily to be removed. This could represent misidentified specimens, etc.\nFormat data: Subset, specify data source as ALA, indicate sensitive species, assign habitat column, rearrange and name columns.\nStudy region: Important if you need to crop data to the region or to work out the current range, background region, and projection region.\nAssign quality levels to data: If you need help with specific data, assigning quality level metrics for taxonomy and geographical dimensions can be good.\nManually identify and remove false positives: False positives that may have been overlooked by automated error removal, based on the knowledge that they are in the records.\nRemove records with an individual count of less than 1 or more than 99: Records may be unsuitable if the number of recorded individuals is 0 or the count is too high (data entry or data-basing problems), indicate records from dna barcoding and in some cases indicate records of absence.\nReach out and ask questions: When preparing occurrence data for modeling it can be helpful to speak to experts in the field."
  },
  {
    "objectID": "preparation.html#extra-r-packages-that-can-help-with-data-preparation",
    "href": "preparation.html#extra-r-packages-that-can-help-with-data-preparation",
    "title": "2  Step 1. Data download and preparation",
    "section": "2.8 EXTRA: R packages that can help with data preparation?",
    "text": "2.8 EXTRA: R packages that can help with data preparation?"
  }
]