---
title: "Binary Classification in R"
author: "Fonti Kar"
date: "2022-10-24"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidymodels, yardstick, rpart, ranger, randomForest, rattle, rpart.plot, RColorBrewer)
```

### Why {tidymodels}

In R, there are multiple packages that fit the same type of model. It is common for each package to provide a unique interface. In other words, things such as an argument for the same model attribute is defined differently for each package. For example, the ranger and randomForest packages fit Random Forest models. In the ranger() function, to define the number of trees we use num.trees. In randomForest, that argument is named ntree. It is not easy to switch between packages to run the same model.

Instead of replacing the modeling package, tidymodels replaces the interface. Better said, tidymodels provides a single set of functions and arguments to define a model.

### Relevant resources

- https://jmsallan.netlify.app/blog/a-workflow-for-binary-classification-with-tidymodels/
- [Continuous predictors](https://medium.com/the-researchers-guide/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055)
- [Decision trees with rpart](https://www.gormanalysis.com/blog/decision-trees-in-r-using-rpart/) 
- [A tutorial with training set](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/) 

# Decision Trees

https://parsnip.tidymodels.org/reference/decision_tree.html

## An example using {rpart}

https://www.gormanalysis.com/blog/decision-trees-in-r-using-rpart/

First let’s define a problem. There’s a common scam amongst motorists whereby a person will slam on his breaks in heavy traffic with the intention of being rear-ended. The person will then file an insurance claim for personal injury and damage to his vehicle, alleging that the other driver was at fault. Suppose we want to predict which of an insurance company’s claims are fraudulent using a decision tree.

```{r}
# Data
train <- data.frame(
  ClaimID = c(1,2,3),
  RearEnd = c(TRUE, FALSE, TRUE),
  Fraud = c(TRUE, FALSE, TRUE)
)


mytree <- rpart(
  Fraud ~ RearEnd, 
  data = train, 
  method = "class"
)

mytree
```

### Minimum observations

Notice the output shows only a root node. This is because rpart has some default parameters that prevented our tree from growing. Namely:

- `minsplit`  is “the minimum number of observations that must exist in a node in order for a split to be attempted” 
- `minbucket.`  “the minimum number of observations in any terminal node”. 

See what happens when we override these parameters.

```{r}
mytree <- rpart(
  Fraud ~ RearEnd, 
  data = train, 
  method = "class", 
  minsplit = 2, 
  minbucket = 1
)

mytree

fancyRpartPlot(mytree, caption = NULL)
```

### Methods in splitting branches

By default, rpart uses [gini impurity](https://www.gormanalysis.com/blog/magic-behind-constructing-a-decision-tree) to select splits when performing classification. (If you’re unfamiliar read this article.) You can use information gain instead by specifying it in the parms parameter.

```{r}
mytree <- rpart(
  Fraud ~ RearEnd, 
  data = train, 
  method = "class",
  parms = list(split = 'information'), 
  minsplit = 2, 
  minbucket = 1
)

mytree
```

### Supplying a training set

```{r}
train_2 <- data.frame(
  ClaimID = c(1,2,3),
  RearEnd = c(TRUE, FALSE, TRUE),
  Fraud = c(TRUE, FALSE, FALSE)
)

train; train_2
```

Build a tree based on new data

The complexity measure is a combination of the size of a tree and the ability of the tree to separate the classes of the target variable. If the next best split in growing a tree does not reduce the tree’s overall complexity by a certain amount, rpart will terminate the growing process. This amount is specified by the complexity parameter, cp, in the call to rpart(). Setting cp to a negative amount ensures that the tree will be fully

```{r}
mytree_2 <- rpart(
  Fraud ~ RearEnd, 
  data = train, 
  method = "class", 
  minsplit = 2, 
  minbucket = 1, 
  cp = -1
)

fancyRpartPlot(mytree_2, caption = NULL)
```

This is not always a good idea since it will typically produce over-fitted trees, but trees can be pruned back as discussed later in this article.

### Weighting each observation

You can also weight each observation for the tree’s construction by specifying the weights argument to rpart()

```{r}
mytree_3 <- rpart(
  Fraud ~ RearEnd, 
  data = train, 
  method = "class", 
  minsplit = 2, 
  minbucket = 1,
  weights = c(0.4, 0.4, 0.2)
)

fancyRpartPlot(mytree_3, caption = NULL)
```

One of the best ways to identify a fraudulent claim is to hire a private investigator to monitor the activities of a claimant. Since private investigators don’t work for free, the insurance company will have to strategically decide which claims to investigate. To do this, they can use a decision tree model based off some initial features of the claim. If the insurance company wants to aggressively investigate claims (i.e. investigate a lot of claims), they can train their decision tree in a manner that will penalize incorrectly labeled fraudulent claims more than it penalizes incorrectly labeled non-fraudulent claims.

### Penalities

I think here `Whiplash` can tell us whether the claim is true or not

```{r}
train_3 <- data.frame(
  ClaimID = 1:7,
  RearEnd = c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE),
  Whiplash = c(TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE),
  Fraud = c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
)

train_3

mytree_4 <- rpart(
  Fraud ~ RearEnd + Whiplash, 
  data = train_3, 
  method = "class",
  maxdepth = 1, 
  minsplit = 2, 
  minbucket = 1
)

fancyRpartPlot(mytree_4, caption = NULL)
```

rpart has determined that RearEnd was the best variable for identifying a fraudulent claim. BUT there was one fraudulent claim in the training dataset that was not a rear-end collision. (ROW 3) If the insurance company wants to identify a high percentage of fraudulent claims without worrying too much about investigating non-fraudulent claims they can set the loss matrix to penalize claims incorrectly labeled as fraudulent three times less than claims incorrectly labeled as non-fraudulent.

To alter the default, equal penalization of mislabeled target classes, set the loss component of the parms parameter to a matrix where the (i,j) element is the penalty for misclassifying an i as a j. (The loss matrix must have 0s in the diagonal). 

```{r}
lossmatrix <- matrix(c(0,1,3,0), byrow = TRUE, nrow = 2)
lossmatrix

mytree_5 <- rpart(
  Fraud ~ RearEnd + Whiplash, 
  data = train_3, 
  method = "class",
  maxdepth = 1, 
  minsplit = 2, 
  minbucket = 1,
  parms = list(loss = lossmatrix)
)

fancyRpartPlot(mytree_5, caption = NULL)
```


# Randpom Forest Trees

## An example first using `iris`

When using tidymodels in binary classification problems, the target variable:

- must be a factor,
- with its first level corresponding to the positive class. i.e 1 is indicator of presence or IS versicolour

```{r}
iris <- iris %>% 
  mutate(is_versicolor = ifelse(Species == "versicolor", "versicolor", "not_versicolor")) %>% # Binary response
  mutate(is_versicolor =factor(is_versicolor, levels = c("versicolor", "not_versicolor"))) # Setting as factor
```

### Pre-processing using the {recipes} package

- https://recipes.tidymodels.org/
- https://www.tidymodels.org/start/recipes/

```{r}
iris_recipe <- iris %>%
  recipe(is_versicolor ~.) %>% # outcome == is_versicolor, predictor is what remains in iris
  step_rm(Species) %>% # removes Species column
  step_corr(all_predictors()) %>% # a correlation filter
  step_center(all_predictors(), -all_outcomes()) %>% # centering predictors but not outcome
  step_scale(all_predictors(), -all_outcomes()) # scaling (z transform) predictors not outcome


iris_recipe |>  prep()
```

### Define tree with {parsnip}

The engine in the parsnip context is the source of the code to run the model. It can be a package, a R base function, stan or spark, among others. Here we are using the random forest implemented in the ranger package.

```{r}
rf <- rand_forest(mode = "classification", trees = 100) %>%
  set_engine("ranger")

rf
```

Alternatively, if we now want to run the same model against randomForest, we simply change the value in set_engine() to “randomForest”.

Snippet from [here](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

```{r}
iris_rf <-  rand_forest(trees = 100, mode = "classification") %>%
  set_engine("randomForest") 
```

### Creating a workflow

```{r}
iris_rf_wf <- workflow() %>%
  add_recipe(iris_recipe) %>%
  add_model(rf)

iris_rf_wf2 <- workflow() %>%
  add_recipe(iris_recipe) %>%
  add_model(iris_rf)
```

### Getting predictions

```{r}
set.seed(3131)

# ranger
iris_pred <- iris_rf_wf %>%
  fit(iris) %>%
  predict(iris) %>%
  bind_cols(iris)

iris_pred %>% glimpse()

set.seed(3131)

# randomForest
iris_pred2 <-iris_rf_wf2 %>%
  fit(iris) %>%
  predict(iris) %>%
  bind_cols(iris)
```

### Model performance evaluations

For more measures: https://yardstick.tidymodels.org/

#### Confusion matrix

Interesting randomForest get ever so slightly  better than ranger

```{r}
# ranger
iris_pred %>%
  conf_mat(truth = is_versicolor, estimate = .pred_class)

# randomForest
iris_pred2 %>%
  conf_mat(truth = is_versicolor, estimate = .pred_class)
```

#### Class metrics

- accuracy: the fraction of observations correctly classified,
- sensibility: the fraction of positive observations correctly classified,
- specificity: the fraction of negative observations correctly classified.

```{r}
class_metrics <- metric_set(accuracy, sens, spec)
```

```{r}
# ranger
iris_pred %>%
 class_metrics(truth = is_versicolor, estimate = .pred_class)

# randomForest
iris_pred2 %>%
 class_metrics(truth = is_versicolor, estimate = .pred_class)
```

### Curve methods

Not sure what these are , but mentioned [here](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

Setting `type = "prob"`, we can get the probability of `versicolour` and `not_versicolour`. 

Checkout ?gain_curve. "If truth is binary, only 1 column should be selected. Otherwise, there should be as many columns as factor levels of truth."

Then we can plot gain curves

```{r}
#ranger
iris_rf_wf %>%
  fit(iris) %>%
  predict(iris, type = "prob") %>%
  bind_cols(iris) -> iris_rf_wf_prob


?gain_curve

iris_rf_wf_prob |> 
  gain_curve(is_versicolor, .pred_versicolor) |> 
  autoplot()
  
```

